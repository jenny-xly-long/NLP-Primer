# NLP-Primer
Natural Language Processing (NLP) is a tract of Artificial Intelligence and Linguistics devoted to make computers understand statements in human languages. Some useful text and speech processing includes conversational agent, dialogue system, machine translation, question answering, etc.
In this document, I will discuss 4 fields in natural language processing:
1. Information Extraction
2. Text Summarization
3. Machine Translation
4. Text Mining

Before we start to talk about the 4 subfields, let's first discuss the different representations of words in NLP. The mean idea is to transform words into numerical representations so that a computer can read it. </br>

**1. Dictionary** </br>
This is a simple approach which essentially assign each word in a given training corpus to an integer. However, because of the ordinal value attached to each words, training models tend to assume that words associated with higher value have more importance which may lead to failure of the model.

**2. One-Hot Encoding** </br>
This method transforms each of the words in a given training corpus to a 1 x (N+1) dimensional vector, where N corresponds to the number of words in the corpus and the added 1 will be used to denote any words that are out of the training vocabulary. However, this method creates sparse matrix with huge dimensions which requires enormous computation power. In addition, it does not take the semantic similarity of words into consideration.

**3. Distributional Representation** </br>
One of the most used example is TF-IDF(Term Frequency-Inverse Document Frequency). This method takes the context and the words appearing in the context into consideration. This way, each sentence/document is represented by a unique vector depending on the number of occurrences of each word in the sentences/documents.

Can we do better? Yes, with neural based representation of words! </br>

**Word2Vec** is a method to construct word embeddings to generate word representations. It generally uses 2 methods: **Skip Gram** and **Continuous Bag Of Words (CBOW)**. Essentially, Word2Vec is a 2-layer neural network. Its direct task is to predict the neighbouring words of a given word. The neural network takes as input one-hot encoded vectors representing each word in a corpus. The output of the network is a single vector containing, for every word in the input vocabulary, the probability that a randomly selected nearby word is this vocabulary word. The indirect task is to learn the weight matrix, which is ultimately th vector representation of the corpus.

So the most commonly used word embedding dimension is 300. This is not an arbitrary number, but rather a hyperparameter of the Word2Vec model. Through empirical experiment, 300 dimensional word embedding vectors perform the best. </br>

![skip_gram_net_arch](https://user-images.githubusercontent.com/30851539/63123223-7c104d00-bf76-11e9-8b7c-d361891ecdd8.png) </br>
(http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) </br>

So what does each dimension signify? The 300 dimensions correspond to the number of "neurons" in the hidden layer of the neural network. We can think of each dimension as a feature of the word. For example, the vector [King] would have features such as *royalty*, *masculinity*, *power*, *human*, ect.; the vector [man] would have features *masculinity*, *human*, etc. and [woman] would have features like *femininity*, *human*, etc. Therefore, by simple substraction, we get </br>
  
[Queen] = [King] - [man] + [woman] = *royalty*, *power*, *femininity*, *human*, etc. </br>

**GLoVE (Global Vectors)** is another distributed word representation. While Word2Vec captures certain local context window, GloVe exploits overall co-occurrence statistics of words from corpus, which is a large collection of texts.

Both Word2Vec and GLoVE incorporate semantic information and word similarities into their embeddings. However, these two methods have limitations when generalizing to words that were not in the original training corpus. Furthermore, if the same word has different meanings in different context, these 2 models do not differentiate the context, the word will have one embedding. To counter these shortcomings, the models ELMo (Embeddings from Language Models) and FastText can be used for high quality embeddings of words.

## Information Extraction
Information extraction (IE) enables automated retrieval of specific information from text. IE depends on Name Entity Recognition to find targeted information to extract, such as LOCATION, PERSON, ORGANIZATION, etc. Once the entities are recognized, algorithms such as relationship extraction, co-reference resolution can then be used to extract its meaning.

### Name Entity Recognition (NER)
NER can automatically scan an entire article and reveal key personnels, organization, location, etc. mentioned in the text. It can also be used to distinguish different meaning of the same term such as Apple the fruit Vs. Apple the company.

The techniques used to perform NER can be characterized in 3 different categories: rule based, feature based and neural based.
#### 1. Rule based
Since ruled based approach is not as often used in industry or in academia, I will not go into too much detail. They are usually used in some very domain specific problems where an exact set of rules and some smaller amount of supervised machine learning can be used to successfully recognized the given entity. For example, if we want to recognize the email **sender**, email **receiver** and email **content** from the following email string
<pre><code> From: John Smith \n To: Jim Bob \n Hello Jim, I am writing to wish you a happy birthday. Best, John </code></pre>
We can perform the following:
<pre><code>import re
sender = re.search(r'From:(.*?)\n', s).group(1) # Get strings between From: and \n
receiver = re.search(r'To:(.*?)\n', s).group(1) # Get strings between To: and \n
content = re.search(r'.*\n(.*)$',s).group(1) # Get strings after the last occurence of \n </code></pre>

This will print:
<pre><code>Sender:  John Smith  
Receiver:  Jim Bob  
Content:  Hello Jim, I am writing to wish you a happy birthday. Best, John </code></pre>

#### 2. Feature based 
This approach is to extract features and train a Conditional Random Fields (CRF) sequence model of the same type as part-of-speech (POS) tagging.

#### 3. Neural based

### Keyword Extraction (KE)
Keyword extraction is an important task in information extraction. It automatically identifies terms that have the most significance or best describes the text. Multiple techniques can be used to extract keywords, one of the most straight forward way to do so is to use Term Frequency - Inverse Document Frequency (TF-IDF). This method is quite intuitive in the sense that it scores the importance of a word by considering its number of occurrence in the given document/sentence and in the entire corpus of documents/sentences. Naturally, if a word occurs frequently in one document, it has high weight/importance; if a word occurs in all document, it is considered as a generic word, thus less important. Here I will present three other ways to perform the task.

#### 1. Using POS tagging and chunking
We can use POS tagger to extract keywords. Given the following example:<br/>
```When former San Francisco 49ers quarterback Colin Kaepernick took a knee during the national anthem in August 2016, it was in protest of unjust police killings of black Americans.For his courage, Kaepernick lost his job, the NFL lost its mind by forbidding the peaceful action ― and meanwhile, at least 378 black Americans have lost their lives in police killings. That most recent estimate of black police violence victims comes from data compiled by The Washington Post and analyzed by HuffPost.Last August, HuffPost reported that based on the Post’s data, at least 223 black Americans had been killed by police gunfire in the year since Kaepernick first sat, then took a knee, to protest police violence. Less than a year later, that number has increased by at least 155 people.```

After preprocessing, use NLTK POS-tagger tool to generate the follwoing word-tag pairs:
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>When</th>      <th>former</th>      <th>San</th>      <th>Francisco</th>      <th>49ers</th>      <th>quarterback</th>      <th>Colin</th>      <th>Kaepernick</th>      <th>took</th>      <th>knee</th>      <th>national</th>      <th>anthem</th>      <th>August</th>      <th>2016</th>      <th>protest</th>      <th>unjust</th>      <th>police</th>      <th>killings</th>      <th>black</th>      <th>Americans</th>      <th>For</th>      <th>courage</th>      <th>lost</th>      <th>job</th>      <th>NFL</th>      <th>mind</th>      <th>forbidding</th>      <th>peaceful</th>      <th>action</th>      <th>meanwhile</th>      <th>least</th>      <th>378</th>      <th>lives</th>      <th>That</th>      <th>recent</th>      <th>estimate</th>      <th>violence</th>      <th>victims</th>      <th>comes</th>      <th>data</th>      <th>compiled</th>      <th>The</th>      <th>Washington</th>      <th>Post</th>      <th>analyzed</th>      <th>HuffPost</th>      <th>Last</th>      <th>reported</th>      <th>based</th>      <th>223</th>      <th>killed</th>      <th>gunfire</th>      <th>year</th>      <th>since</th>      <th>first</th>      <th>sat</th>      <th>Less</th>      <th>later</th>      <th>number</th>      <th>increased</th>      <th>155</th>      <th>people</th>    </tr>  </thead>  <tbody>    <tr>      <th>Tags</th>      <td>WRB</td>      <td>JJ</td>      <td>NNP</td>      <td>NNP</td>      <td>CD</td>      <td>NN</td>      <td>NNP</td>      <td>NNP</td>      <td>VBD</td>      <td>NN</td>      <td>JJ</td>      <td>NN</td>      <td>NNP</td>      <td>CD</td>      <td>NN</td>      <td>JJ</td>      <td>NN</td>      <td>NNS</td>      <td>JJ</td>      <td>NNPS</td>      <td>IN</td>      <td>NN</td>      <td>VBN</td>      <td>NN</td>      <td>NNP</td>      <td>NN</td>      <td>VBG</td>      <td>JJ</td>      <td>NN</td>      <td>RB</td>      <td>JJS</td>      <td>CD</td>      <td>NNS</td>      <td>WDT</td>      <td>JJ</td>      <td>NN</td>      <td>NN</td>      <td>NNS</td>      <td>VBZ</td>      <td>NNS</td>      <td>VBD</td>      <td>DT</td>      <td>NNP</td>      <td>NNP</td>      <td>VBD</td>      <td>NNP</td>      <td>JJ</td>      <td>VBD</td>      <td>VBN</td>      <td>CD</td>      <td>VBN</td>      <td>NN</td>      <td>NN</td>      <td>IN</td>      <td>RB</td>      <td>VBD</td>      <td>NNP</td>      <td>RB</td>      <td>NN</td>      <td>VBD</td>      <td>CD</td>      <td>NNS</td>    </tr>  </tbody></table>
<br/>

Now let's perform name entity (NE) chunking to add more structure/name entity tags to the sentences. The NE-chunk classifier is supposed to recognize PERSON, ORGANISATION, etc. Here are the top level tags and their meaning:<br/>
> - geo = Geographical Entity
> - org = Organization
> - per = Person
> - gpe = Geopolitical Entity
> - tim = Time indicator
> - art = Artifact
> - eve = Event
> - nat = Natural Phenomenon
<br/>
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>6</th>      <th>7</th>      <th>8</th>      <th>9</th>      <th>10</th>      <th>11</th>      <th>12</th>      <th>13</th>      <th>14</th>      <th>15</th>      <th>16</th>      <th>17</th>      <th>18</th>      <th>19</th>      <th>20</th>      <th>21</th>      <th>22</th>      <th>23</th>      <th>24</th>      <th>25</th>      <th>26</th>      <th>27</th>      <th>28</th>      <th>29</th>      <th>30</th>      <th>31</th>      <th>32</th>      <th>33</th>      <th>34</th>      <th>35</th>      <th>36</th>      <th>37</th>      <th>38</th>      <th>39</th>      <th>40</th>      <th>41</th>      <th>42</th>      <th>43</th>      <th>44</th>      <th>45</th>      <th>46</th>      <th>47</th>      <th>48</th>      <th>49</th>      <th>50</th>      <th>51</th>      <th>52</th>      <th>53</th>      <th>54</th>      <th>55</th>      <th>56</th>      <th>57</th>      <th>58</th>      <th>59</th>      <th>60</th>      <th>61</th>      <th>62</th>      <th>63</th>      <th>64</th>      <th>65</th>      <th>66</th>      <th>67</th>      <th>68</th>      <th>69</th>      <th>70</th>      <th>71</th>      <th>72</th>      <th>73</th>      <th>74</th>      <th>75</th>      <th>76</th>      <th>77</th>      <th>78</th>      <th>79</th>      <th>80</th>      <th>81</th>      <th>82</th>      <th>83</th>      <th>84</th>      <th>85</th>      <th>86</th>    </tr>  </thead>  <tbody>    <tr>      <th>Words</th>      <td>When</td>      <td>former</td>      <td>San</td>      <td>Francisco</td>      <td>49ers</td>      <td>quarterback</td>      <td>Colin</td>      <td>Kaepernick</td>      <td>took</td>      <td>knee</td>      <td>national</td>      <td>anthem</td>      <td>August</td>      <td>2016</td>      <td>protest</td>      <td>unjust</td>      <td>police</td>      <td>killings</td>      <td>black</td>      <td>Americans</td>      <td>For</td>      <td>courage</td>      <td>Kaepernick</td>      <td>lost</td>      <td>job</td>      <td>NFL</td>      <td>lost</td>      <td>mind</td>      <td>forbidding</td>      <td>peaceful</td>      <td>action</td>      <td>meanwhile</td>      <td>least</td>      <td>378</td>      <td>black</td>      <td>Americans</td>      <td>lost</td>      <td>lives</td>      <td>police</td>      <td>killings</td>      <td>That</td>      <td>recent</td>      <td>estimate</td>      <td>black</td>      <td>police</td>      <td>violence</td>      <td>victims</td>      <td>comes</td>      <td>data</td>      <td>compiled</td>      <td>The</td>      <td>Washington</td>      <td>Post</td>      <td>analyzed</td>      <td>HuffPost</td>      <td>Last</td>      <td>August</td>      <td>HuffPost</td>      <td>reported</td>      <td>based</td>      <td>Post</td>      <td>data</td>      <td>least</td>      <td>223</td>      <td>black</td>      <td>Americans</td>      <td>killed</td>      <td>police</td>      <td>gunfire</td>      <td>year</td>      <td>since</td>      <td>Kaepernick</td>      <td>first</td>      <td>sat</td>      <td>took</td>      <td>knee</td>      <td>protest</td>      <td>police</td>      <td>violence</td>      <td>Less</td>      <td>year</td>      <td>later</td>      <td>number</td>      <td>increased</td>      <td>least</td>      <td>155</td>      <td>people</td>    </tr>    <tr>      <th>POS Tags</th>      <td>WRB</td>      <td>JJ</td>      <td>NNP</td>      <td>NNP</td>      <td>CD</td>      <td>NN</td>      <td>NNP</td>      <td>NNP</td>      <td>VBD</td>      <td>NNP</td>      <td>JJ</td>      <td>NN</td>      <td>NNP</td>      <td>CD</td>      <td>NN</td>      <td>JJ</td>      <td>NN</td>      <td>NNS</td>      <td>JJ</td>      <td>NNPS</td>      <td>IN</td>      <td>NN</td>      <td>NNP</td>      <td>VBD</td>      <td>NN</td>      <td>NNP</td>      <td>VBD</td>      <td>NN</td>      <td>VBG</td>      <td>JJ</td>      <td>NN</td>      <td>RB</td>      <td>JJS</td>      <td>CD</td>      <td>JJ</td>      <td>NNPS</td>      <td>VBN</td>      <td>NNS</td>      <td>JJ</td>      <td>NNS</td>      <td>WDT</td>      <td>JJ</td>      <td>NN</td>      <td>JJ</td>      <td>NN</td>      <td>NN</td>      <td>NNS</td>      <td>VBZ</td>      <td>NNS</td>      <td>VBD</td>      <td>DT</td>      <td>NNP</td>      <td>NNP</td>      <td>VBD</td>      <td>NNP</td>      <td>JJ</td>      <td>NNP</td>      <td>NNP</td>      <td>VBD</td>      <td>VBN</td>      <td>NNP</td>      <td>NNS</td>      <td>JJS</td>      <td>CD</td>      <td>JJ</td>      <td>NNPS</td>      <td>VBN</td>      <td>NN</td>      <td>NN</td>      <td>NN</td>      <td>IN</td>      <td>NNP</td>      <td>RB</td>      <td>VBD</td>      <td>VBD</td>      <td>NN</td>      <td>NN</td>      <td>NN</td>      <td>NN</td>      <td>NNP</td>      <td>NN</td>      <td>RB</td>      <td>NN</td>      <td>VBD</td>      <td>JJS</td>      <td>CD</td>      <td>NNS</td>    </tr>    <tr>      <th>IOB Tags</th>      <td>O</td>      <td>O</td>      <td>B-GPE</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>B-PERSON</td>      <td>I-PERSON</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>B-PERSON</td>      <td>O</td>      <td>O</td>      <td>B-ORGANIZATION</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>B-ORGANIZATION</td>      <td>I-ORGANIZATION</td>      <td>O</td>      <td>B-ORGANIZATION</td>      <td>O</td>      <td>O</td>      <td>B-ORGANIZATION</td>      <td>O</td>      <td>O</td>      <td>B-ORGANIZATION</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>B-PERSON</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>B-PERSON</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>      <td>O</td>    </tr>  </tbody></table>
<br/>
As seen in the "IOB Tag" row, the tagger mostly successfully picked up different name entities in the given text. We can then extract those with the NE tags as keywords. Using SpaCy's **displacy** visualization library, we get the following graph:

![tagged_sentence](https://user-images.githubusercontent.com/30851539/62237415-e6ce5f80-b39e-11e9-8d78-d729bb03d219.gif)


#### 2. Using TextRank
TextRank is a graph-based ranking model for text processing, especially for keyword and sentence extraction (Rada and Tarau, 2004). This methode is similar to Google's PageRank algorithm (Brin and Page, 1998). Intuitively, TextRank works well because it does not only rely on the local context of a text unit, but it takes into account information recursively drawn from the entire text. It is able to identify connections between various entities in a text and implements the concept of recommendation.
<br/>

TextRank can be implemented through Python's *summa* libray. For example, we take the same text as before and we get the following keywords:
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>6</th>      <th>7</th>    </tr>  </thead>  <tbody>    <tr>      <th>Keywords</th>      <td>police</td>      <td>lost</td>      <td>post</td>      <td>kaepernick took</td>      <td>black</td>      <td>protest</td>      <td>data</td>      <td>huffpost</td>    </tr>  </tbody></table>
<br/>

Building on top of the TextRank idea, we can illustrate a keywords relation graph using Liu's implementation (https://github.com/liuhuanyong/TextGrapher). The above text example results in the following text graph.
![keyword_graph](https://user-images.githubusercontent.com/30851539/62246646-15096a80-b3b2-11e9-87e7-7781535f6929.gif)

#### 3. Unsupervised





### Topic Modeling
Topic modeling is an unsupervised technique to discover topics across various text documents. It has various applications in different domain. For instance, historians can use it to identify important events in history; web based libraries can use it to recommend books based on your past readings. News providers can leverage the technique to understand articles quickly or to group similar articles.

#### 1. Latent Semantic Analysis (LSA)
#### 2. Latent Dirichlet Allocation (LDA)
LDA is a Bayesian model that uses Dirichlet priors to compute document-topic and word-topic distributions. It has two basic assumptions: 
```
1. Each document consists of a mixture of topics
2. Each topic consists of a set of words
```
LDA considers each document as a collection of topics of different weights. And each topic as a collection of keywords in a certain proportion. Therefore, from LDA's perspective, a topic is nothing but a collection of representative, dominant keywords. You can then infer the topic by looking at the collection of keywords. <br/>

**Logic of LDA:**
<pre><code>1. Specify the number of topics *N* to discover (note that this is a hyperparameter of the model that we need to fine tune)
2. Randomly assign each word of the documents to one the the *N* topics.
3. Compute the product of P(Topic *T* | Document *D*) [probability of documents *D* assigned to topic *T*] and P(Word *w* | Topic *T*)[probability of topic *T* assignment contributed by word *w*].
4. Re-assign a new topic to each word according to the above result which represents the likelihood that topic *T* would generate word *w*.
5. Repeat steps 3 and 4 until we reach a steady state where topics are assigned correctly according to the computed probability.</code></pre>

**Example of LDA implementation:**
LDA can be implemented using Sklean or gensim library. I am using gensim and wordnet on the News Category(attach link) dataset. Please see "lda.py" for detailed code. The goal here is to further understand LDA through some code and visualization.

Here is a snapshot of the data set<br/>

<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>category</th>      <th>headline</th>      <th>index</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>CRIME</td>      <td>There Were 2 Mass Shootings In Texas Last Week...</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>ENTERTAINMENT</td>      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>ENTERTAINMENT</td>      <td>Hugh Grant Marries For The First Time At Age 57</td>      <td>2</td>    </tr>    <tr>      <th>3</th>      <td>ENTERTAINMENT</td>      <td>Jim Carrey Blasts \'Castrato\' Adam Schiff And D...</td>      <td>3</td>    </tr>    <tr>      <th>4</th>      <td>ENTERTAINMENT</td>      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>      <td>4</td>    </tr>    <tr>      <th>5</th>      <td>ENTERTAINMENT</td>      <td>Morgan Freeman \'Devastated\' That Sexual Harass...</td>      <td>5</td>    </tr>    <tr>      <th>6</th>      <td>ENTERTAINMENT</td>      <td>Donald Trump Is Lovin\' New McDonald\'s Jingle I...</td>      <td>6</td>    </tr>    <tr>      <th>7</th>      <td>ENTERTAINMENT</td>      <td>What To Watch On Amazon Prime That’s New This ...</td>      <td>7</td>    </tr>    <tr>      <th>8</th>      <td>ENTERTAINMENT</td>      <td>Mike Myers Reveals He\'d \'Like To\' Do A Fourth ...</td>      <td>8</td>    </tr>    <tr>      <th>9</th>      <td>ENTERTAINMENT</td>      <td>What To Watch On Hulu That’s New This Week</td>      <td>9</td>    </tr>  </tbody></table>


We first preprocess the data by removing the stopwords, tokenizing, lemmatizig and stemming the text.<br/>
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>headline</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>[mass, shoot, texa, week]</td>    </tr>    <tr>      <th>1</th>      <td>[smith, join, diplo, nicki, world, offici, song]</td>    </tr>    <tr>      <th>2</th>      <td>[hugh, grant, marri, time]</td>    </tr>    <tr>      <th>3</th>      <td>[carrey, blast, castrato, adam, schiff, democr...</td>    </tr>    <tr>      <th>4</th>      <td>[julianna, marguli, use, donald, trump, poop, ...</td>    </tr>    <tr>      <th>5</th>      <td>[morgan, freeman, devast, sexual, harass, clai...</td>    </tr>    <tr>      <th>6</th>      <td>[donald, trump, lovin, mcdonald, jingl, tonight]</td>    </tr>    <tr>      <th>7</th>      <td>[watch, amazon, prime, week]</td>    </tr>    <tr>      <th>8</th>      <td>[mike, myer, reveal, like, fourth, austin, pow...</td>    </tr>    <tr>      <th>9</th>      <td>[watch, hulu, week]</td>    </tr>  </tbody></table>

We then match each of the words to a word ID and store them in a dictionary, shown as following<br/>
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>6</th>      <th>7</th>      <th>8</th>      <th>9</th>      <th>10</th>      <th>11</th>      <th>12</th>      <th>13</th>      <th>14</th>      <th>15</th>      <th>16</th>      <th>17</th>      <th>18</th>      <th>19</th>      <th>20</th>      <th>21</th>      <th>22</th>      <th>23</th>      <th>24</th>      <th>25</th>      <th>26</th>      <th>27</th>      <th>28</th>      <th>29</th>      <th>30</th>      <th>31</th>      <th>32</th>      <th>33</th>      <th>34</th>      <th>35</th>      <th>36</th>      <th>37</th>      <th>38</th>      <th>39</th>      <th>40</th>      <th>41</th>      <th>42</th>      <th>43</th>      <th>44</th>      <th>45</th>      <th>46</th>      <th>47</th>      <th>48</th>      <th>49</th>      <th>50</th>      <th>51</th>      <th>52</th>      <th>53</th>      <th>54</th>      <th>55</th>      <th>56</th>      <th>57</th>      <th>58</th>      <th>59</th>      <th>60</th>      <th>61</th>      <th>62</th>      <th>63</th>      <th>64</th>      <th>65</th>      <th>66</th>      <th>67</th>      <th>68</th>      <th>69</th>      <th>70</th>      <th>71</th>      <th>72</th>      <th>73</th>      <th>74</th>      <th>75</th>      <th>76</th>      <th>77</th>      <th>78</th>      <th>79</th>      <th>80</th>      <th>81</th>      <th>82</th>      <th>83</th>      <th>84</th>      <th>85</th>      <th>86</th>      <th>87</th>      <th>88</th>      <th>89</th>      <th>90</th>      <th>91</th>      <th>92</th>      <th>93</th>      <th>94</th>      <th>95</th>      <th>96</th>      <th>97</th>      <th>98</th>      <th>99</th>    </tr>  </thead>  <tbody>    <tr>      <th>Words</th>      <td>mass</td>      <td>shoot</td>      <td>texa</td>      <td>week</td>      <td>diplo</td>      <td>join</td>      <td>nicki</td>      <td>offici</td>      <td>smith</td>      <td>song</td>      <td>world</td>      <td>grant</td>      <td>hugh</td>      <td>marri</td>      <td>time</td>      <td>adam</td>      <td>artwork</td>      <td>blast</td>      <td>carrey</td>      <td>castrato</td>      <td>democrat</td>      <td>schiff</td>      <td>bag</td>      <td>donald</td>      <td>julianna</td>      <td>marguli</td>      <td>pick</td>      <td>poop</td>      <td>trump</td>      <td>use</td>      <td>claim</td>      <td>devast</td>      <td>freeman</td>      <td>harass</td>      <td>legaci</td>      <td>morgan</td>      <td>sexual</td>      <td>undermin</td>      <td>jingl</td>      <td>lovin</td>      <td>mcdonald</td>      <td>tonight</td>      <td>amazon</td>      <td>prime</td>      <td>watch</td>      <td>austin</td>      <td>film</td>      <td>fourth</td>      <td>like</td>      <td>mike</td>      <td>myer</td>      <td>power</td>      <td>reveal</td>      <td>hulu</td>      <td>justin</td>      <td>school</td>      <td>timberlak</td>      <td>victim</td>      <td>visit</td>      <td>jong</td>      <td>korea</td>      <td>korean</td>      <td>meet</td>      <td>north</td>      <td>presid</td>      <td>south</td>      <td>summit</td>      <td>talk</td>      <td>call</td>      <td>grow</td>      <td>life</td>      <td>oyster</td>      <td>region</td>      <td>remot</td>      <td>risk</td>      <td>robot</td>      <td>crackdown</td>      <td>immigr</td>      <td>kid</td>      <td>parent</td>      <td>put</td>      <td>strain</td>      <td>alli</td>      <td>concern</td>      <td>obtain</td>      <td>putin</td>      <td>wiretap</td>      <td>edward</td>      <td>love</td>      <td>snowden</td>      <td>vladimir</td>      <td>booyah</td>      <td>hilari</td>      <td>obama</td>      <td>photograph</td>      <td>troll</td>      <td>abort</td>      <td>amend</td>      <td>ireland</td>      <td>landslid</td>    </tr>  </tbody></table>

Here is one example from the data set, we can see if any of the words in the dictionary occured in this sentence and we can count their number of occurrence.<br/>
```Police killed at least 378 black Americans from the moment Colin Kaepernick protested```
<br/>
> Word 149 ("protest") appears 1 time.<br/>
> Word 157 ("american") appears 1 time.<br/>
> Word 158 ("black") appears 1 time.<br/>
> Word 159 ("colin") appears 1 time.<br/>
> Word 160 ("kaepernick") appears 1 time.<br/>
> Word 161 ("kill") appears 1 time.<br/>
> Word 162 ("moment") appears 1 time.<br/>
> Word 163 ("polic") appears 1 time.

Finally, we build the LDA model by choosing the number of topics. Here I made an "ansatz" that there are 40 distinct topics since there are 40 different categories associated to the dataset. Some of the topics are shown as below:<br/>
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>6</th>      <th>7</th>      <th>8</th>      <th>9</th>      <th>10</th>      <th>11</th>      <th>12</th>      <th>13</th>      <th>14</th>      <th>15</th>      <th>16</th>      <th>17</th>      <th>18</th>      <th>19</th>      <th>20</th>      <th>21</th>      <th>22</th>      <th>23</th>      <th>24</th>      <th>25</th>      <th>26</th>      <th>27</th>      <th>28</th>      <th>29</th>      <th>30</th>      <th>31</th>      <th>32</th>      <th>33</th>      <th>34</th>      <th>35</th>      <th>36</th>      <th>37</th>      <th>38</th>      <th>39</th>    </tr>  </thead>  <tbody>    <tr>      <th>Topics</th>      <td>0.157*"week" + 0.064*"photo" + 0.060*"summer" ...</td>      <td>0.109*"style" + 0.059*"photo" + 0.029*"teen" +...</td>      <td>0.075*"babi" + 0.057*"fall" + 0.057*"photo" + ...</td>      <td>0.069*"right" + 0.041*"feel" + 0.030*"light" +...</td>      <td>0.060*"lose" + 0.045*"heart" + 0.033*"mind" + ...</td>      <td>0.070*"beauti" + 0.069*"star" + 0.060*"photo" ...</td>      <td>0.083*"photo" + 0.034*"cover" + 0.032*"post" +...</td>      <td>0.049*"photo" + 0.038*"bodi" + 0.022*"danger" ...</td>      <td>0.174*"wed" + 0.037*"video" + 0.024*"photo" + ...</td>      <td>0.094*"fashion" + 0.093*"studi" + 0.054*"chang...</td>      <td>0.078*"photo" + 0.074*"parent" + 0.068*"dress"...</td>      <td>0.047*"plan" + 0.042*"parti" + 0.041*"super" +...</td>      <td>0.100*"life" + 0.040*"learn" + 0.037*"real" + ...</td>      <td>0.036*"cook" + 0.034*"workout" + 0.032*"challe...</td>      <td>0.061*"healthi" + 0.057*"secret" + 0.042*"vale...</td>      <td>0.095*"travel" + 0.032*"propos" + 0.031*"olymp...</td>      <td>0.130*"food" + 0.056*"coupl" + 0.037*"mean" + ...</td>      <td>0.097*"health" + 0.049*"hair" + 0.044*"care" +...</td>      <td>0.096*"sleep" + 0.028*"doctor" + 0.027*"name" ...</td>      <td>0.076*"marriag" + 0.045*"child" + 0.035*"date"...</td>      <td>0.103*"kid" + 0.050*"guid" + 0.048*"inspir" + ...</td>      <td>0.055*"children" + 0.041*"money" + 0.029*"film...</td>      <td>0.032*"histori" + 0.029*"self" + 0.027*"makeup...</td>      <td>0.044*"daughter" + 0.038*"street" + 0.031*"wal...</td>      <td>0.054*"girl" + 0.049*"holiday" + 0.041*"video"...</td>      <td>0.109*"way" + 0.063*"cancer" + 0.053*"design" ...</td>      <td>0.067*"citi" + 0.052*"worst" + 0.039*"york" + ...</td>      <td>0.057*"better" + 0.038*"link" + 0.031*"deal" +...</td>      <td>0.204*"best" + 0.041*"photo" + 0.029*"person" ...</td>      <td>0.128*"divorc" + 0.040*"model" + 0.034*"weeken...</td>      <td>0.066*"poll" + 0.051*"save" + 0.035*"question"...</td>      <td>0.093*"thing" + 0.080*"know" + 0.079*"need" + ...</td>      <td>0.060*"open" + 0.058*"great" + 0.037*"pound" +...</td>      <td>0.089*"work" + 0.061*"weight" + 0.039*"loss" +...</td>      <td>0.078*"tip" + 0.048*"photo" + 0.040*"spring" +...</td>      <td>0.042*"power" + 0.041*"favorit" + 0.038*"yoga"...</td>      <td>0.085*"recip" + 0.053*"photo" + 0.053*"america...</td>      <td>0.055*"school" + 0.050*"black" + 0.037*"high" ...</td>      <td>0.064*"idea" + 0.041*"marri" + 0.031*"vintag" ...</td>      <td>0.055*"stop" + 0.030*"suggest" + 0.024*"spot" ...</td>    </tr>  </tbody></table>

I put the first 9 topics into wordclouds for better visualization. The size of the word is proportionate to its weight/significance in the given sentence.<br/>

![lda40_wordcloud](https://user-images.githubusercontent.com/30851539/62183034-b1862b00-b326-11e9-9f55-58968b39df8d.png)

As we can see, each topic is just a combination of keywords. For the chosen example, its assigned topics are:<br/>
![lda_example](https://user-images.githubusercontent.com/30851539/62182499-ccf03680-b324-11e9-9355-28fc098605be.png)

We can then infer its topic may be "Police shoots and kills black lives".
Here is a great visualization of LDA created by pyLDAvis library. Ideally, the topic bubble should not be overlapping. If the topics are too crowded, it may be an indicator that the number-of-topic parameter is set too high.<br/>

![lda_vis](https://user-images.githubusercontent.com/30851539/62246841-82b59680-b3b2-11e9-9d9d-1d59d25a94c7.gif)


### Event Extraction (EE)
EE gathers knowledge about periodical incidents found in texts, automatically identifying information about what happed and when it happened. The amount of text generated daily are enormous. Being able to extract key information from a giant pool of data can help us be more efficient. For example, extracting events from business news aids users to perceive market trends, stay up to date with competitors' strategies and to make valuable investment decisioins.

Let's take the following Google News dataset that I found here(insert link). We want to extract event from the descriptions of each news article. The column "desc" is a preprocessed version of column "description".
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>category</th>      <th>date_publish</th>      <th>title</th>      <th>description</th>      <th>desc</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>North Korea Missile</td>      <td>2017-11-06T22:18:21</td>      <td>Trump says Japan would shoot North Korean missiles \'out of sky\' if it bought U.S. weaponry</td>      <td>U.S. President Donald Trump said on Monday that Japan would shoot North Korean missiles "out of the sky" if it bought the U.S. weaponry needed for doing so, suggesting Tokyo take a stance it has avoided until now.</td>      <td>president donald trump said monday japan would shoot north korean missiles sky bought weaponry needed suggesting tokyo take stance avoided</td>    </tr>    <tr>      <th>1</th>      <td>Equifax breach</td>      <td>2017-11-14T10:01:09</td>      <td>How Much Will Equifax Pay?</td>      <td>A law that protects consumers’ data was written before the age of hacking.</td>      <td>law protects consumers data written age hacking</td>    </tr>    <tr>      <th>2</th>      <td>Dieselgate</td>      <td>2017-10-24T08:11:27</td>      <td>EU raids automaker BMW in post-Dieselgate cartel case</td>      <td>EU antitrust regulators have raided the offices of automaker BMW in Munich, the company said Friday, in a fresh blow to the beleaguered German car industry.</td>      <td>antitrust regulators raided offices automaker bmw munich company said friday fresh blow beleaguered german car industry</td>    </tr>    <tr>      <th>3</th>      <td>Zimbabwe</td>      <td>2017-11-16T00:00:00</td>      <td>Zimbabwe calm as Mugabe is urged to go peacefully</td>      <td>Zimbabwe\'s President Robert Mugabe was meeting with a South African delegation at the state house today as negotiations pushed for a resolution to the political turmoil and the likely end to his decades-long rule.</td>      <td>zimbabwe president robert mugabe meeting south african delegation state house today negotiations pushed resolution political turmoil likely end decades long rule</td>    </tr>    <tr>      <th>4</th>      <td>Zimbabwe</td>      <td>2017-11-17T00:00:00</td>      <td>Zimbabwe human rights lawyer: Nothing is going to change</td>      <td>Nothing is going to change in Zimbabwe‚ because President Robert Mugabe and the military leaders who seized power made decisions together for the last 37 years.</td>      <td>nothing going change zimbabwe president robert mugabe military leaders seized power made decisions together last years</td>    </tr>  </tbody></table>

We can then vectorize the description sentences using SpaCy's pre-trained word embeddings *en_core_web_lg*. Here is the result, as you can see, each sentence is now corresponding to a 300 dimensional vector.
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>6</th>      <th>7</th>      <th>8</th>      <th>9</th>      <th>10</th>      <th>11</th>      <th>12</th>      <th>13</th>      <th>14</th>      <th>15</th>      <th>16</th>      <th>17</th>      <th>18</th>      <th>19</th>      <th>20</th>      <th>21</th>      <th>22</th>      <th>23</th>      <th>24</th>      <th>25</th>      <th>26</th>      <th>27</th>      <th>28</th>      <th>29</th>      <th>30</th>      <th>31</th>      <th>32</th>      <th>33</th>      <th>34</th>      <th>35</th>      <th>36</th>      <th>37</th>      <th>38</th>      <th>39</th>      <th>40</th>      <th>41</th>      <th>42</th>      <th>43</th>      <th>44</th>      <th>45</th>      <th>46</th>      <th>47</th>      <th>48</th>      <th>49</th>      <th>50</th>      <th>51</th>      <th>52</th>      <th>53</th>      <th>54</th>      <th>55</th>      <th>56</th>      <th>57</th>      <th>58</th>      <th>59</th>      <th>60</th>      <th>61</th>      <th>62</th>      <th>63</th>      <th>64</th>      <th>65</th>      <th>66</th>      <th>67</th>      <th>68</th>      <th>69</th>      <th>70</th>      <th>71</th>      <th>72</th>      <th>73</th>      <th>74</th>      <th>75</th>      <th>76</th>      <th>77</th>      <th>78</th>      <th>79</th>      <th>80</th>      <th>81</th>      <th>82</th>      <th>83</th>      <th>84</th>      <th>85</th>      <th>86</th>      <th>87</th>      <th>88</th>      <th>89</th>      <th>90</th>      <th>91</th>      <th>92</th>      <th>93</th>      <th>94</th>      <th>95</th>      <th>96</th>      <th>97</th>      <th>98</th>      <th>99</th>      <th>100</th>      <th>101</th>      <th>102</th>      <th>103</th>      <th>104</th>      <th>105</th>      <th>106</th>      <th>107</th>      <th>108</th>      <th>109</th>      <th>110</th>      <th>111</th>      <th>112</th>      <th>113</th>      <th>114</th>      <th>115</th>      <th>116</th>      <th>117</th>      <th>118</th>      <th>119</th>      <th>120</th>      <th>121</th>      <th>122</th>      <th>123</th>      <th>124</th>      <th>125</th>      <th>126</th>      <th>127</th>      <th>128</th>      <th>129</th>      <th>130</th>      <th>131</th>      <th>132</th>      <th>133</th>      <th>134</th>      <th>135</th>      <th>136</th>      <th>137</th>      <th>138</th>      <th>139</th>      <th>140</th>      <th>141</th>      <th>142</th>      <th>143</th>      <th>144</th>      <th>145</th>      <th>146</th>      <th>147</th>      <th>148</th>      <th>149</th>      <th>150</th>      <th>151</th>      <th>152</th>      <th>153</th>      <th>154</th>      <th>155</th>      <th>156</th>      <th>157</th>      <th>158</th>      <th>159</th>      <th>160</th>      <th>161</th>      <th>162</th>      <th>163</th>      <th>164</th>      <th>165</th>      <th>166</th>      <th>167</th>      <th>168</th>      <th>169</th>      <th>170</th>      <th>171</th>      <th>172</th>      <th>173</th>      <th>174</th>      <th>175</th>      <th>176</th>      <th>177</th>      <th>178</th>      <th>179</th>      <th>180</th>      <th>181</th>      <th>182</th>      <th>183</th>      <th>184</th>      <th>185</th>      <th>186</th>      <th>187</th>      <th>188</th>      <th>189</th>      <th>190</th>      <th>191</th>      <th>192</th>      <th>193</th>      <th>194</th>      <th>195</th>      <th>196</th>      <th>197</th>      <th>198</th>      <th>199</th>      <th>200</th>      <th>201</th>      <th>202</th>      <th>203</th>      <th>204</th>      <th>205</th>      <th>206</th>      <th>207</th>      <th>208</th>      <th>209</th>      <th>210</th>      <th>211</th>      <th>212</th>      <th>213</th>      <th>214</th>      <th>215</th>      <th>216</th>      <th>217</th>      <th>218</th>      <th>219</th>      <th>220</th>      <th>221</th>      <th>222</th>      <th>223</th>      <th>224</th>      <th>225</th>      <th>226</th>      <th>227</th>      <th>228</th>      <th>229</th>      <th>230</th>      <th>231</th>      <th>232</th>      <th>233</th>      <th>234</th>      <th>235</th>      <th>236</th>      <th>237</th>      <th>238</th>      <th>239</th>      <th>240</th>      <th>241</th>      <th>242</th>      <th>243</th>      <th>244</th>      <th>245</th>      <th>246</th>      <th>247</th>      <th>248</th>      <th>249</th>      <th>250</th>      <th>251</th>      <th>252</th>      <th>253</th>      <th>254</th>      <th>255</th>      <th>256</th>      <th>257</th>      <th>258</th>      <th>259</th>      <th>260</th>      <th>261</th>      <th>262</th>      <th>263</th>      <th>264</th>      <th>265</th>      <th>266</th>      <th>267</th>      <th>268</th>      <th>269</th>      <th>270</th>      <th>271</th>      <th>272</th>      <th>273</th>      <th>274</th>      <th>275</th>      <th>276</th>      <th>277</th>      <th>278</th>      <th>279</th>      <th>280</th>      <th>281</th>      <th>282</th>      <th>283</th>      <th>284</th>      <th>285</th>      <th>286</th>      <th>287</th>      <th>288</th>      <th>289</th>      <th>290</th>      <th>291</th>      <th>292</th>      <th>293</th>      <th>294</th>      <th>295</th>      <th>296</th>      <th>297</th>      <th>298</th>      <th>299</th>    </tr>  </thead>  <tbody>    <tr>      <th>president donald trump said monday japan would shoot north korean missiles sky bought weaponry needed suggesting tokyo take stance avoided</th>      <td>-0.066091</td>      <td>0.008265</td>      <td>0.053317</td>      <td>-0.055286</td>      <td>0.004674</td>      <td>-0.079551</td>      <td>-0.152034</td>      <td>0.090800</td>      <td>-0.116715</td>      <td>1.759943</td>      <td>-0.251980</td>      <td>-0.205502</td>      <td>0.120570</td>      <td>-0.057488</td>      <td>-0.176745</td>      <td>-0.134804</td>      <td>-0.096458</td>      <td>0.598214</td>      <td>-0.070077</td>      <td>0.047188</td>      <td>0.169122</td>      <td>0.061276</td>      <td>0.089337</td>      <td>-0.180614</td>      <td>-0.219261</td>      <td>0.061051</td>      <td>-0.175278</td>      <td>-0.156899</td>      <td>-0.049068</td>      <td>-0.033868</td>      <td>-0.053627</td>      <td>-0.048977</td>      <td>0.057018</td>      <td>0.100686</td>      <td>-0.078915</td>      <td>0.031116</td>      <td>0.000668</td>      <td>0.061282</td>      <td>-0.020022</td>      <td>-0.021665</td>      <td>-0.124327</td>      <td>-0.070767</td>      <td>-0.007653</td>      <td>0.118538</td>      <td>0.083477</td>      <td>0.018813</td>      <td>-0.165741</td>      <td>-0.152180</td>      <td>0.014063</td>      <td>0.013149</td>      <td>-0.106389</td>      <td>0.044184</td>      <td>-0.121803</td>      <td>0.057659</td>      <td>-0.031382</td>      <td>0.017160</td>      <td>-0.209682</td>      <td>-0.030572</td>      <td>0.007354</td>      <td>-0.209549</td>      <td>-0.226624</td>      <td>-0.043396</td>      <td>0.094602</td>      <td>-0.047151</td>      <td>0.170221</td>      <td>-0.031212</td>      <td>-0.125862</td>      <td>0.162613</td>      <td>0.024869</td>      <td>-0.073180</td>      <td>-0.167181</td>      <td>-0.003054</td>      <td>-0.083431</td>      <td>0.203847</td>      <td>0.012279</td>      <td>-0.073647</td>      <td>-0.073846</td>      <td>-0.107847</td>      <td>-0.046465</td>      <td>0.196246</td>      <td>-0.137921</td>      <td>-0.226922</td>      <td>-0.028953</td>      <td>0.162165</td>      <td>0.041880</td>      <td>-0.141398</td>      <td>0.297829</td>      <td>-0.183785</td>      <td>0.069091</td>      <td>-0.088027</td>      <td>0.048894</td>      <td>0.013741</td>      <td>-0.133588</td>      <td>-0.226995</td>      <td>0.073583</td>      <td>-0.027828</td>      <td>0.109126</td>      <td>0.004161</td>      <td>-0.059502</td>      <td>0.078516</td>      <td>0.000515</td>      <td>0.114843</td>      <td>-0.013253</td>      <td>0.187216</td>      <td>0.078208</td>      <td>-1.044518</td>      <td>0.078826</td>      <td>-0.079839</td>      <td>-0.125495</td>      <td>-0.010823</td>      <td>0.076521</td>      <td>-0.156424</td>      <td>-0.047132</td>      <td>0.002078</td>      <td>0.041965</td>      <td>0.060034</td>      <td>0.190691</td>      <td>0.029134</td>      <td>-0.026263</td>      <td>-0.045787</td>      <td>0.053989</td>      <td>0.229230</td>      <td>-0.010374</td>      <td>0.027254</td>      <td>0.062769</td>      <td>0.024635</td>      <td>0.031548</td>      <td>0.041938</td>      <td>-0.119157</td>      <td>0.036523</td>      <td>-0.014705</td>      <td>-0.068379</td>      <td>0.029682</td>      <td>-0.137084</td>      <td>-0.029028</td>      <td>-0.189539</td>      <td>0.032747</td>      <td>-0.012962</td>      <td>-0.084978</td>      <td>-0.025945</td>      <td>-1.410527</td>      <td>-0.077860</td>      <td>0.021723</td>      <td>-0.073304</td>      <td>-0.031308</td>      <td>0.127510</td>      <td>0.007732</td>      <td>0.032388</td>      <td>-0.072924</td>      <td>-0.108049</td>      <td>0.011602</td>      <td>-0.025889</td>      <td>-0.061529</td>      <td>0.117414</td>      <td>-0.083262</td>      <td>-0.158413</td>      <td>-0.102192</td>      <td>0.002711</td>      <td>-0.032150</td>      <td>0.169368</td>      <td>0.108285</td>      <td>-0.007611</td>      <td>-0.153804</td>      <td>0.132687</td>      <td>-0.090170</td>      <td>0.011410</td>      <td>-0.031249</td>      <td>-0.044014</td>      <td>0.146038</td>      <td>0.076288</td>      <td>-0.083291</td>      <td>0.080040</td>      <td>-0.107490</td>      <td>-0.022098</td>      <td>-0.063035</td>      <td>-0.060466</td>      <td>-0.019399</td>      <td>0.190809</td>      <td>0.057025</td>      <td>-0.031506</td>      <td>-0.021047</td>      <td>-0.064312</td>      <td>-0.076688</td>      <td>-0.040054</td>      <td>0.008970</td>      <td>0.032749</td>      <td>0.045950</td>      <td>-0.047431</td>      <td>0.052181</td>      <td>0.110828</td>      <td>0.188410</td>      <td>0.001803</td>      <td>-0.118360</td>      <td>-0.168467</td>      <td>-0.051810</td>      <td>-0.017688</td>      <td>0.031907</td>      <td>-0.078162</td>      <td>0.074558</td>      <td>0.000010</td>      <td>0.036177</td>      <td>0.002127</td>      <td>0.189360</td>      <td>-0.071029</td>      <td>0.142733</td>      <td>0.007234</td>      <td>0.019685</td>      <td>-0.084936</td>      <td>0.172805</td>      <td>0.050689</td>      <td>0.015998</td>      <td>-0.068785</td>      <td>0.130874</td>      <td>-0.065268</td>      <td>0.170738</td>      <td>0.013795</td>      <td>-0.149168</td>      <td>0.021878</td>      <td>-0.042603</td>      <td>0.062805</td>      <td>0.037934</td>      <td>-0.083372</td>      <td>-0.009548</td>      <td>-0.073781</td>      <td>-0.110153</td>      <td>-0.045056</td>      <td>-0.034098</td>      <td>0.069663</td>      <td>-0.024798</td>      <td>0.155709</td>      <td>-0.013370</td>      <td>0.039483</td>      <td>0.030936</td>      <td>-0.169758</td>      <td>0.040576</td>      <td>-0.003454</td>      <td>0.073719</td>      <td>-0.027552</td>      <td>0.143023</td>      <td>0.096013</td>      <td>-0.028950</td>      <td>-0.067065</td>      <td>0.090886</td>      <td>-0.007246</td>      <td>-0.006559</td>      <td>-0.023785</td>      <td>0.015884</td>      <td>0.070397</td>      <td>-0.091897</td>      <td>-0.079272</td>      <td>0.008964</td>      <td>0.043137</td>      <td>0.043028</td>      <td>-0.133464</td>      <td>0.001719</td>      <td>0.073715</td>      <td>-0.177578</td>      <td>0.057787</td>      <td>-0.188085</td>      <td>-0.112994</td>      <td>0.046084</td>      <td>-0.002545</td>      <td>-0.094821</td>      <td>0.029267</td>      <td>-0.109437</td>      <td>-0.144996</td>      <td>0.006209</td>      <td>0.096015</td>      <td>-0.072390</td>      <td>0.192097</td>      <td>-0.032154</td>      <td>0.137033</td>      <td>-0.057067</td>      <td>0.108012</td>      <td>0.081820</td>      <td>0.094789</td>      <td>-0.000199</td>      <td>-0.120879</td>      <td>-0.020908</td>      <td>0.079873</td>      <td>0.181896</td>      <td>-0.123121</td>      <td>-0.050850</td>      <td>0.134507</td>      <td>0.023026</td>      <td>0.024214</td>      <td>-0.025953</td>      <td>-0.044892</td>      <td>0.033102</td>      <td>0.063353</td>      <td>-0.016880</td>      <td>0.054314</td>      <td>-0.042788</td>      <td>0.019930</td>      <td>0.065284</td>      <td>0.003405</td>      <td>-0.155270</td>      <td>-0.006865</td>      <td>0.104692</td>      <td>0.219717</td>    </tr>    <tr>      <th>law protects consumers data written age hacking</th>      <td>-0.250118</td>      <td>0.098807</td>      <td>-0.078441</td>      <td>-0.112370</td>      <td>-0.027300</td>      <td>-0.065117</td>      <td>0.190085</td>      <td>-0.262161</td>      <td>0.096585</td>      <td>2.152200</td>      <td>-0.272370</td>      <td>-0.043793</td>      <td>-0.228566</td>      <td>0.120592</td>      <td>-0.425706</td>      <td>-0.136075</td>      <td>-0.055507</td>      <td>1.231370</td>      <td>0.084503</td>      <td>-0.068995</td>      <td>0.156650</td>      <td>-0.036977</td>      <td>0.007996</td>      <td>-0.003682</td>      <td>0.145606</td>      <td>-0.039898</td>      <td>0.022530</td>      <td>0.042758</td>      <td>-0.072100</td>      <td>0.009796</td>      <td>-0.260229</td>      <td>0.070671</td>      <td>0.054266</td>      <td>-0.163578</td>      <td>0.097628</td>      <td>0.034903</td>      <td>-0.288066</td>      <td>0.084441</td>      <td>0.176582</td>      <td>0.031944</td>      <td>0.080145</td>      <td>0.071873</td>      <td>0.048180</td>      <td>-0.192065</td>      <td>-0.289337</td>      <td>0.205040</td>      <td>-0.188446</td>      <td>-0.008731</td>      <td>0.312299</td>      <td>-0.034642</td>      <td>-0.274182</td>      <td>-0.089731</td>      <td>-0.178877</td>      <td>-0.022260</td>      <td>-0.197252</td>      <td>0.001935</td>      <td>0.035875</td>      <td>-0.169540</td>      <td>-0.016158</td>      <td>-0.157204</td>      <td>0.256111</td>      <td>0.124366</td>      <td>0.123875</td>      <td>0.263629</td>      <td>0.132283</td>      <td>-0.179602</td>      <td>0.037967</td>      <td>0.073453</td>      <td>0.225869</td>      <td>-0.005790</td>      <td>-0.088429</td>      <td>-0.060712</td>      <td>0.360486</td>      <td>-0.100967</td>      <td>0.065142</td>      <td>-0.287133</td>      <td>0.271960</td>      <td>0.105956</td>      <td>-0.108737</td>      <td>0.205083</td>      <td>0.145299</td>      <td>0.117263</td>      <td>0.145654</td>      <td>0.111430</td>      <td>0.082123</td>      <td>-0.107174</td>      <td>-0.104852</td>      <td>-0.178004</td>      <td>0.221365</td>      <td>0.189943</td>      <td>0.013412</td>      <td>-0.012451</td>      <td>-0.128160</td>      <td>0.101094</td>      <td>0.117050</td>      <td>-0.178088</td>      <td>0.010523</td>      <td>0.129214</td>      <td>0.089852</td>      <td>-0.172835</td>      <td>0.238534</td>      <td>0.000277</td>      <td>0.000939</td>      <td>0.184593</td>      <td>-0.238319</td>      <td>-1.406550</td>      <td>-0.158428</td>      <td>-0.001880</td>      <td>0.160871</td>      <td>0.012905</td>      <td>0.243121</td>      <td>0.099435</td>      <td>-0.039751</td>      <td>-0.155770</td>      <td>-0.338073</td>      <td>0.138731</td>      <td>-0.109967</td>      <td>0.103023</td>      <td>0.031332</td>      <td>-0.107837</td>      <td>-0.039667</td>      <td>-0.015971</td>      <td>-0.126876</td>      <td>-0.189392</td>      <td>0.040684</td>      <td>0.271954</td>      <td>0.258619</td>      <td>-0.078975</td>      <td>0.055599</td>      <td>0.066015</td>      <td>-0.003895</td>      <td>0.203163</td>      <td>0.208617</td>      <td>-0.057068</td>      <td>0.018464</td>      <td>-0.054003</td>      <td>-0.207483</td>      <td>-0.122707</td>      <td>0.077829</td>      <td>0.163840</td>      <td>-0.310612</td>      <td>-0.088983</td>      <td>0.270897</td>      <td>-0.022580</td>      <td>0.000822</td>      <td>-0.001897</td>      <td>-0.067451</td>      <td>-0.003638</td>      <td>-0.222444</td>      <td>-0.006327</td>      <td>0.100801</td>      <td>-0.176771</td>      <td>0.119369</td>      <td>-0.019891</td>      <td>-0.011879</td>      <td>0.186917</td>      <td>-0.140604</td>      <td>0.050900</td>      <td>-0.139208</td>      <td>-0.058749</td>      <td>0.152030</td>      <td>0.091291</td>      <td>0.238461</td>      <td>-0.018470</td>      <td>-0.420394</td>      <td>0.052503</td>      <td>0.112987</td>      <td>-0.000385</td>      <td>0.061767</td>      <td>-0.023717</td>      <td>0.128536</td>      <td>-0.065659</td>      <td>0.155345</td>      <td>0.121915</td>      <td>0.091165</td>      <td>0.054024</td>      <td>0.025071</td>      <td>-0.126404</td>      <td>-0.004901</td>      <td>0.087038</td>      <td>-0.016983</td>      <td>-0.107550</td>      <td>-0.007745</td>      <td>-0.057552</td>      <td>0.058582</td>      <td>0.277768</td>      <td>-0.085019</td>      <td>0.007294</td>      <td>0.024738</td>      <td>0.031272</td>      <td>-0.125432</td>      <td>-0.217981</td>      <td>-0.195353</td>      <td>0.226934</td>      <td>0.067357</td>      <td>0.060717</td>      <td>-0.123626</td>      <td>-0.008558</td>      <td>0.026265</td>      <td>0.063034</td>      <td>0.024588</td>      <td>-0.243434</td>      <td>0.035270</td>      <td>0.195894</td>      <td>-0.031471</td>      <td>0.243235</td>      <td>-0.002789</td>      <td>0.060537</td>      <td>-0.089933</td>      <td>-0.104764</td>      <td>0.090476</td>      <td>-0.189017</td>      <td>-0.247914</td>      <td>-0.163839</td>      <td>0.052620</td>      <td>-0.086731</td>      <td>-0.122483</td>      <td>-0.089636</td>      <td>-0.457634</td>      <td>0.040278</td>      <td>-0.006511</td>      <td>0.171626</td>      <td>-0.090372</td>      <td>-0.020407</td>      <td>0.004931</td>      <td>-0.058520</td>      <td>0.020686</td>      <td>0.052592</td>      <td>0.208645</td>      <td>-0.170036</td>      <td>-0.214954</td>      <td>0.023463</td>      <td>0.011173</td>      <td>0.003209</td>      <td>-0.114053</td>      <td>0.181089</td>      <td>0.091111</td>      <td>-0.199047</td>      <td>0.006237</td>      <td>-0.115087</td>      <td>0.238152</td>      <td>-0.132757</td>      <td>0.253235</td>      <td>-0.088418</td>      <td>0.126447</td>      <td>-0.217881</td>      <td>-0.009571</td>      <td>-0.075894</td>      <td>-0.024916</td>      <td>0.031149</td>      <td>0.196509</td>      <td>0.035196</td>      <td>-0.098477</td>      <td>-0.054139</td>      <td>-0.182740</td>      <td>0.088099</td>      <td>-0.119306</td>      <td>-0.004969</td>      <td>0.247654</td>      <td>-0.002652</td>      <td>0.077641</td>      <td>0.325993</td>      <td>-0.017943</td>      <td>0.115074</td>      <td>-0.166147</td>      <td>-0.097217</td>      <td>-0.091205</td>      <td>0.046626</td>      <td>0.718196</td>      <td>-0.263373</td>      <td>-0.072844</td>      <td>-0.023098</td>      <td>-0.027499</td>      <td>0.059720</td>      <td>0.289506</td>      <td>-0.127851</td>      <td>-0.112127</td>      <td>0.084084</td>      <td>-0.063081</td>      <td>0.018463</td>      <td>-0.081044</td>      <td>0.156052</td>      <td>0.260497</td>      <td>0.041946</td>      <td>0.072091</td>      <td>0.081532</td>      <td>-0.026487</td>      <td>-0.023012</td>      <td>0.154437</td>      <td>-0.189526</td>      <td>-0.488531</td>      <td>-0.210992</td>      <td>-0.135236</td>      <td>0.164730</td>      <td>0.120123</td>      <td>-0.009834</td>      <td>-0.026736</td>      <td>0.129183</td>      <td>0.036385</td>      <td>0.112868</td>    </tr>    <tr>      <th>antitrust regulators raided offices automaker bmw munich company said friday fresh blow beleaguered german car industry</th>      <td>-0.011729</td>      <td>0.239839</td>      <td>0.129662</td>      <td>-0.020705</td>      <td>0.078966</td>      <td>-0.269153</td>      <td>-0.120838</td>      <td>-0.100197</td>      <td>-0.082086</td>      <td>1.648590</td>      <td>-0.262009</td>      <td>-0.264287</td>      <td>0.080589</td>      <td>-0.261017</td>      <td>-0.168640</td>      <td>-0.114491</td>      <td>0.036258</td>      <td>0.596501</td>      <td>0.180629</td>      <td>-0.059966</td>      <td>0.208806</td>      <td>0.147585</td>      <td>0.286604</td>      <td>-0.119829</td>      <td>-0.082993</td>      <td>0.089486</td>      <td>-0.279518</td>      <td>-0.066441</td>      <td>0.071051</td>      <td>-0.008671</td>      <td>-0.011401</td>      <td>-0.088867</td>      <td>0.053676</td>      <td>0.236881</td>      <td>0.198058</td>      <td>0.086178</td>      <td>-0.110195</td>      <td>0.100539</td>      <td>-0.041003</td>      <td>0.004344</td>      <td>0.052855</td>      <td>-0.176556</td>      <td>0.104357</td>      <td>0.109326</td>      <td>-0.049138</td>      <td>-0.005592</td>      <td>0.072368</td>      <td>-0.246513</td>      <td>0.006814</td>      <td>0.005450</td>      <td>-0.062422</td>      <td>0.114902</td>      <td>-0.182282</td>      <td>0.176065</td>      <td>-0.055969</td>      <td>-0.071771</td>      <td>-0.184493</td>      <td>-0.140081</td>      <td>0.090558</td>      <td>-0.160842</td>      <td>0.093458</td>      <td>-0.198807</td>      <td>0.025275</td>      <td>0.120193</td>      <td>0.031335</td>      <td>0.037117</td>      <td>0.029352</td>      <td>0.094345</td>      <td>-0.019196</td>      <td>0.056517</td>      <td>-0.089390</td>      <td>-0.105171</td>      <td>-0.046811</td>      <td>0.121694</td>      <td>-0.062939</td>      <td>-0.120200</td>      <td>0.031269</td>      <td>0.099191</td>      <td>-0.049394</td>      <td>0.084426</td>      <td>-0.005433</td>      <td>0.074749</td>      <td>-0.027258</td>      <td>-0.173289</td>      <td>-0.175132</td>      <td>-0.070778</td>      <td>0.130361</td>      <td>0.142878</td>      <td>-0.006188</td>      <td>0.073778</td>      <td>0.039346</td>      <td>-0.062404</td>      <td>0.002157</td>      <td>-0.198648</td>      <td>0.172041</td>      <td>-0.111169</td>      <td>-0.079019</td>      <td>-0.095432</td>      <td>0.063608</td>      <td>0.073481</td>      <td>-0.150227</td>      <td>0.022363</td>      <td>0.027330</td>      <td>0.156305</td>      <td>0.010101</td>      <td>-1.172255</td>      <td>0.163712</td>      <td>0.035267</td>      <td>-0.027811</td>      <td>0.071921</td>      <td>0.043287</td>      <td>-0.016030</td>      <td>-0.003504</td>      <td>-0.005176</td>      <td>-0.089568</td>      <td>0.021205</td>      <td>0.341535</td>      <td>-0.005535</td>      <td>0.090683</td>      <td>-0.080000</td>      <td>0.060302</td>      <td>0.087374</td>      <td>-0.005446</td>      <td>-0.263672</td>      <td>-0.259956</td>      <td>0.140834</td>      <td>0.163919</td>      <td>-0.036673</td>      <td>-0.055864</td>      <td>0.173782</td>      <td>0.034218</td>      <td>-0.044641</td>      <td>0.129597</td>      <td>-0.127320</td>      <td>0.000818</td>      <td>-0.206031</td>      <td>-0.140413</td>      <td>-0.003545</td>      <td>-0.079719</td>      <td>-0.202208</td>      <td>-0.830705</td>      <td>-0.290163</td>      <td>0.199185</td>      <td>-0.284532</td>      <td>-0.009049</td>      <td>-0.013430</td>      <td>-0.170833</td>      <td>0.179782</td>      <td>-0.015083</td>      <td>0.019115</td>      <td>-0.041762</td>      <td>-0.175752</td>      <td>-0.052113</td>      <td>-0.039176</td>      <td>0.137024</td>      <td>0.040612</td>      <td>-0.176992</td>      <td>0.211370</td>      <td>-0.065132</td>      <td>0.174284</td>      <td>0.066906</td>      <td>-0.061380</td>      <td>-0.032819</td>      <td>-0.120349</td>      <td>-0.019844</td>      <td>-0.144141</td>      <td>-0.030549</td>      <td>-0.004610</td>      <td>0.056295</td>      <td>0.004078</td>      <td>-0.030816</td>      <td>-0.072864</td>      <td>-0.266726</td>      <td>0.087252</td>      <td>0.071831</td>      <td>-0.159315</td>      <td>-0.029664</td>      <td>0.189604</td>      <td>0.150682</td>      <td>0.136507</td>      <td>-0.021176</td>      <td>0.011407</td>      <td>-0.121302</td>      <td>-0.150564</td>      <td>-0.245039</td>      <td>-0.078954</td>      <td>0.026670</td>      <td>0.177856</td>      <td>-0.057693</td>      <td>0.117533</td>      <td>0.071398</td>      <td>0.044957</td>      <td>0.090342</td>      <td>0.013561</td>      <td>-0.064198</td>      <td>-0.124136</td>      <td>0.239051</td>      <td>0.099878</td>      <td>-0.017554</td>      <td>-0.073164</td>      <td>-0.003846</td>      <td>-0.013265</td>      <td>0.106034</td>      <td>-0.159261</td>      <td>0.124367</td>      <td>-0.126236</td>      <td>0.174320</td>      <td>0.235350</td>      <td>0.078637</td>      <td>0.008365</td>      <td>-0.124401</td>      <td>-0.011098</td>      <td>0.115445</td>      <td>0.157809</td>      <td>0.162902</td>      <td>0.092326</td>      <td>0.063198</td>      <td>-0.147415</td>      <td>-0.262123</td>      <td>0.095464</td>      <td>-0.062100</td>      <td>-0.201958</td>      <td>-0.002147</td>      <td>0.124775</td>      <td>-0.027120</td>      <td>-0.114184</td>      <td>-0.041133</td>      <td>0.144256</td>      <td>0.048788</td>      <td>-0.167679</td>      <td>-0.023032</td>      <td>-0.006397</td>      <td>-0.133224</td>      <td>-0.144308</td>      <td>-0.113194</td>      <td>0.010255</td>      <td>-0.109925</td>      <td>-0.120728</td>      <td>0.100707</td>      <td>-0.000344</td>      <td>0.124439</td>      <td>-0.177712</td>      <td>0.240872</td>      <td>0.131907</td>      <td>-0.124046</td>      <td>0.092958</td>      <td>0.095259</td>      <td>-0.038314</td>      <td>0.031843</td>      <td>-0.061284</td>      <td>0.024074</td>      <td>0.022709</td>      <td>0.030287</td>      <td>-0.076494</td>      <td>0.036714</td>      <td>-0.093702</td>      <td>-0.075723</td>      <td>0.141401</td>      <td>-0.078868</td>      <td>-0.104947</td>      <td>0.162112</td>      <td>0.121845</td>      <td>-0.147644</td>      <td>0.033965</td>      <td>-0.074687</td>      <td>-0.368742</td>      <td>0.053082</td>      <td>-0.049231</td>      <td>-0.168843</td>      <td>0.023057</td>      <td>-0.060303</td>      <td>-0.014863</td>      <td>0.237129</td>      <td>-0.001482</td>      <td>0.029760</td>      <td>0.206069</td>      <td>-0.036283</td>      <td>0.050184</td>      <td>0.393155</td>      <td>-0.025352</td>      <td>-0.230923</td>      <td>-0.158207</td>      <td>0.088712</td>      <td>0.309439</td>      <td>-0.092813</td>      <td>0.267340</td>      <td>0.039208</td>      <td>0.040985</td>      <td>-0.108408</td>      <td>0.052408</td>      <td>-0.348700</td>      <td>-0.169576</td>      <td>0.205977</td>      <td>-0.090558</td>      <td>-0.250222</td>      <td>-0.292219</td>      <td>0.187517</td>      <td>-0.134078</td>      <td>-0.016548</td>      <td>0.231435</td>    </tr>    <tr>      <th>zimbabwe president robert mugabe meeting south african delegation state house today negotiations pushed resolution political turmoil likely end decades long rule</th>      <td>0.001461</td>      <td>0.144930</td>      <td>0.173314</td>      <td>-0.029563</td>      <td>0.075130</td>      <td>-0.033254</td>      <td>-0.031601</td>      <td>0.045852</td>      <td>0.107485</td>      <td>2.317000</td>      <td>-0.217034</td>      <td>-0.095032</td>      <td>0.100786</td>      <td>-0.077404</td>      <td>-0.169063</td>      <td>-0.128527</td>      <td>-0.052886</td>      <td>0.457472</td>      <td>-0.063326</td>      <td>0.094890</td>      <td>-0.004973</td>      <td>0.038964</td>      <td>0.171564</td>      <td>-0.104935</td>      <td>-0.010018</td>      <td>0.005018</td>      <td>-0.100833</td>      <td>0.105258</td>      <td>-0.240105</td>      <td>0.138156</td>      <td>0.098524</td>      <td>-0.020051</td>      <td>-0.018604</td>      <td>-0.108343</td>      <td>0.026496</td>      <td>0.093933</td>      <td>0.107981</td>      <td>-0.110406</td>      <td>-0.177934</td>      <td>-0.106217</td>      <td>0.117740</td>      <td>-0.006146</td>      <td>0.145918</td>      <td>0.014604</td>      <td>0.043345</td>      <td>-0.068588</td>      <td>-0.025704</td>      <td>0.015897</td>      <td>-0.147536</td>      <td>-0.136753</td>      <td>-0.101350</td>      <td>0.151293</td>      <td>-0.081104</td>      <td>-0.043198</td>      <td>0.045525</td>      <td>0.094749</td>      <td>-0.108876</td>      <td>-0.015452</td>      <td>0.226676</td>      <td>-0.160299</td>      <td>-0.291879</td>      <td>0.140974</td>      <td>-0.043570</td>      <td>0.041840</td>      <td>-0.001722</td>      <td>0.142848</td>      <td>-0.174953</td>      <td>0.159644</td>      <td>-0.031066</td>      <td>0.022078</td>      <td>0.074462</td>      <td>0.018117</td>      <td>-0.073818</td>      <td>-0.013103</td>      <td>0.056765</td>      <td>0.038346</td>      <td>-0.164323</td>      <td>0.017861</td>      <td>-0.018805</td>      <td>0.079149</td>      <td>0.036697</td>      <td>0.177358</td>      <td>-0.071792</td>      <td>0.119624</td>      <td>-0.201374</td>      <td>-0.087746</td>      <td>0.033291</td>      <td>-0.315107</td>      <td>0.265071</td>      <td>0.032194</td>      <td>-0.071381</td>      <td>-0.097547</td>      <td>-0.037154</td>      <td>-0.081377</td>      <td>0.070786</td>      <td>-0.100733</td>      <td>-0.004475</td>      <td>0.049107</td>      <td>0.073704</td>      <td>-0.076489</td>      <td>0.208106</td>      <td>0.151957</td>      <td>0.019602</td>      <td>0.053861</td>      <td>0.055097</td>      <td>-1.157504</td>      <td>-0.092454</td>      <td>-0.050250</td>      <td>0.002465</td>      <td>0.071581</td>      <td>-0.058115</td>      <td>0.018255</td>      <td>-0.094572</td>      <td>-0.050754</td>      <td>0.020208</td>      <td>-0.083025</td>      <td>0.100409</td>      <td>-0.074872</td>      <td>-0.021057</td>      <td>-0.024466</td>      <td>-0.149134</td>      <td>0.173376</td>      <td>-0.012690</td>      <td>0.121974</td>      <td>-0.059849</td>      <td>0.046916</td>      <td>0.149061</td>      <td>0.085494</td>      <td>-0.091815</td>      <td>0.117717</td>      <td>-0.127247</td>      <td>-0.018767</td>      <td>0.066480</td>      <td>-0.020271</td>      <td>-0.016052</td>      <td>0.023888</td>      <td>-0.015106</td>      <td>0.135923</td>      <td>0.043024</td>      <td>-0.031018</td>      <td>-0.776452</td>      <td>-0.141646</td>      <td>0.016443</td>      <td>-0.131539</td>      <td>0.175212</td>      <td>-0.009921</td>      <td>0.016674</td>      <td>0.099850</td>      <td>-0.199407</td>      <td>0.052639</td>      <td>-0.101882</td>      <td>-0.073665</td>      <td>-0.086339</td>      <td>0.077624</td>      <td>-0.023418</td>      <td>-0.003013</td>      <td>0.186436</td>      <td>0.101569</td>      <td>-0.035709</td>      <td>0.222929</td>      <td>-0.007260</td>      <td>-0.017801</td>      <td>-0.104785</td>      <td>0.086065</td>      <td>-0.247442</td>      <td>0.028462</td>      <td>0.100346</td>      <td>-0.174809</td>      <td>0.144309</td>      <td>0.023011</td>      <td>-0.043903</td>      <td>0.097607</td>      <td>-0.052961</td>      <td>0.050914</td>      <td>-0.132867</td>      <td>-0.051099</td>      <td>-0.130880</td>      <td>0.034705</td>      <td>0.172988</td>      <td>-0.007052</td>      <td>-0.028064</td>      <td>-0.119993</td>      <td>-0.017936</td>      <td>-0.109921</td>      <td>0.041947</td>      <td>0.027970</td>      <td>-0.022493</td>      <td>-0.181257</td>      <td>-0.059083</td>      <td>0.080766</td>      <td>0.029331</td>      <td>-0.041064</td>      <td>-0.070190</td>      <td>-0.048274</td>      <td>0.103959</td>      <td>0.059138</td>      <td>0.015770</td>      <td>0.011177</td>      <td>-0.054235</td>      <td>0.001149</td>      <td>0.105855</td>      <td>0.204887</td>      <td>0.122898</td>      <td>-0.057331</td>      <td>0.117591</td>      <td>-0.102922</td>      <td>0.040331</td>      <td>-0.057511</td>      <td>-0.009033</td>      <td>0.083882</td>      <td>-0.070066</td>      <td>0.049455</td>      <td>0.157672</td>      <td>0.142223</td>      <td>0.174799</td>      <td>0.020234</td>      <td>-0.132064</td>      <td>0.060554</td>      <td>-0.071327</td>      <td>0.029601</td>      <td>-0.123528</td>      <td>0.010181</td>      <td>-0.082248</td>      <td>0.013934</td>      <td>0.014328</td>      <td>0.123109</td>      <td>-0.114631</td>      <td>-0.003515</td>      <td>0.093979</td>      <td>0.157078</td>      <td>-0.072059</td>      <td>0.128205</td>      <td>0.095099</td>      <td>-0.046400</td>      <td>-0.010342</td>      <td>-0.088266</td>      <td>-0.230107</td>      <td>-0.093453</td>      <td>0.009473</td>      <td>0.120570</td>      <td>0.017324</td>      <td>-0.150595</td>      <td>0.179110</td>      <td>0.088378</td>      <td>-0.121869</td>      <td>0.041530</td>      <td>0.189041</td>      <td>0.069549</td>      <td>-0.037989</td>      <td>-0.236331</td>      <td>-0.029823</td>      <td>0.145714</td>      <td>0.128751</td>      <td>-0.024191</td>      <td>0.058506</td>      <td>0.078947</td>      <td>-0.068967</td>      <td>-0.014891</td>      <td>-0.191489</td>      <td>-0.073885</td>      <td>0.094774</td>      <td>0.039962</td>      <td>0.020573</td>      <td>0.231403</td>      <td>-0.024585</td>      <td>-0.310706</td>      <td>-0.024444</td>      <td>-0.110351</td>      <td>0.113083</td>      <td>0.144832</td>      <td>-0.048751</td>      <td>0.060258</td>      <td>-0.121081</td>      <td>-0.004872</td>      <td>0.068395</td>      <td>0.108739</td>      <td>-0.005629</td>      <td>0.121151</td>      <td>0.036649</td>      <td>-0.048100</td>      <td>0.147406</td>      <td>-0.143005</td>      <td>-0.067245</td>      <td>0.283130</td>      <td>0.010339</td>      <td>-0.002080</td>      <td>0.057787</td>      <td>0.030134</td>      <td>-0.035583</td>      <td>0.049475</td>      <td>-0.045338</td>      <td>0.144593</td>      <td>0.056953</td>      <td>-0.038318</td>      <td>0.019244</td>      <td>0.123017</td>      <td>0.009111</td>      <td>-0.075468</td>      <td>-0.124304</td>      <td>-0.052500</td>    </tr>    <tr>      <th>nothing going change zimbabwe president robert mugabe military leaders seized power made decisions together last years</th>      <td>-0.099937</td>      <td>0.222496</td>      <td>0.268552</td>      <td>-0.126975</td>      <td>-0.101139</td>      <td>-0.018168</td>      <td>-0.130718</td>      <td>0.044538</td>      <td>0.043517</td>      <td>2.265738</td>      <td>-0.149759</td>      <td>0.020849</td>      <td>0.097392</td>      <td>-0.110862</td>      <td>-0.164687</td>      <td>-0.243613</td>      <td>-0.124819</td>      <td>0.484228</td>      <td>-0.163160</td>      <td>0.207280</td>      <td>0.178237</td>      <td>-0.095723</td>      <td>0.085761</td>      <td>-0.110205</td>      <td>-0.047178</td>      <td>0.086366</td>      <td>-0.163829</td>      <td>0.016475</td>      <td>-0.107858</td>      <td>0.049809</td>      <td>-0.048613</td>      <td>0.049835</td>      <td>-0.040046</td>      <td>0.039336</td>      <td>0.042840</td>      <td>0.098584</td>      <td>0.115362</td>      <td>-0.063619</td>      <td>-0.043381</td>      <td>-0.081886</td>      <td>0.031383</td>      <td>0.048496</td>      <td>0.149646</td>      <td>-0.044715</td>      <td>0.090798</td>      <td>-0.084552</td>      <td>-0.117117</td>      <td>0.052894</td>      <td>-0.049305</td>      <td>-0.098816</td>      <td>-0.074687</td>      <td>0.114586</td>      <td>0.008870</td>      <td>-0.002796</td>      <td>0.066690</td>      <td>0.081418</td>      <td>-0.053821</td>      <td>-0.088478</td>      <td>0.208785</td>      <td>-0.171579</td>      <td>-0.185192</td>      <td>0.074648</td>      <td>-0.096420</td>      <td>0.049551</td>      <td>0.169126</td>      <td>0.117527</td>      <td>-0.207209</td>      <td>0.121092</td>      <td>-0.018973</td>      <td>-0.093479</td>      <td>0.001858</td>      <td>-0.019039</td>      <td>0.083245</td>      <td>-0.026879</td>      <td>0.082433</td>      <td>0.035634</td>      <td>-0.055510</td>      <td>0.007623</td>      <td>0.056204</td>      <td>0.154886</td>      <td>-0.036405</td>      <td>0.120520</td>      <td>-0.062659</td>      <td>0.051792</td>      <td>-0.163499</td>      <td>-0.114759</td>      <td>-0.000136</td>      <td>-0.445467</td>      <td>0.269464</td>      <td>0.065091</td>      <td>-0.033511</td>      <td>0.008020</td>      <td>-0.120120</td>      <td>-0.107891</td>      <td>0.106981</td>      <td>-0.062279</td>      <td>0.093043</td>      <td>-0.011595</td>      <td>0.005115</td>      <td>-0.143024</td>      <td>0.004253</td>      <td>0.216911</td>      <td>0.073198</td>      <td>0.043316</td>      <td>0.068775</td>      <td>-1.095393</td>      <td>-0.045920</td>      <td>-0.052909</td>      <td>0.040221</td>      <td>0.199806</td>      <td>-0.081264</td>      <td>-0.091381</td>      <td>0.011577</td>      <td>-0.180268</td>      <td>0.015069</td>      <td>-0.027421</td>      <td>0.164301</td>      <td>0.045938</td>      <td>-0.039435</td>      <td>0.086190</td>      <td>-0.073395</td>      <td>0.141729</td>      <td>0.010055</td>      <td>0.179586</td>      <td>-0.020102</td>      <td>0.015814</td>      <td>-0.075298</td>      <td>0.046460</td>      <td>0.043716</td>      <td>0.053545</td>      <td>-0.081124</td>      <td>0.114230</td>      <td>0.028068</td>      <td>-0.070983</td>      <td>-0.075280</td>      <td>-0.067769</td>      <td>0.022892</td>      <td>0.136682</td>      <td>0.002660</td>      <td>0.081492</td>      <td>-0.986492</td>      <td>-0.049231</td>      <td>0.189660</td>      <td>0.060745</td>      <td>0.115400</td>      <td>0.023911</td>      <td>-0.039087</td>      <td>0.050967</td>      <td>-0.213089</td>      <td>-0.059768</td>      <td>-0.077536</td>      <td>-0.135989</td>      <td>-0.015592</td>      <td>0.056471</td>      <td>-0.028523</td>      <td>-0.079309</td>      <td>0.017881</td>      <td>0.017318</td>      <td>0.015335</td>      <td>0.162294</td>      <td>-0.003267</td>      <td>0.076370</td>      <td>-0.010289</td>      <td>0.086442</td>      <td>-0.182392</td>      <td>0.026131</td>      <td>0.083408</td>      <td>-0.246792</td>      <td>0.219594</td>      <td>0.036669</td>      <td>-0.110569</td>      <td>0.054939</td>      <td>0.045174</td>      <td>0.079503</td>      <td>-0.063091</td>      <td>0.021766</td>      <td>-0.018336</td>      <td>0.131682</td>      <td>0.190364</td>      <td>0.047037</td>      <td>0.063820</td>      <td>-0.041192</td>      <td>-0.084272</td>      <td>-0.136828</td>      <td>-0.026982</td>      <td>0.077210</td>      <td>-0.022162</td>      <td>-0.152327</td>      <td>0.052042</td>      <td>0.119155</td>      <td>0.099492</td>      <td>0.095453</td>      <td>0.030885</td>      <td>-0.003194</td>      <td>-0.004757</td>      <td>0.172066</td>      <td>0.121806</td>      <td>-0.053664</td>      <td>-0.008676</td>      <td>0.035001</td>      <td>0.073306</td>      <td>0.031597</td>      <td>0.241615</td>      <td>-0.051987</td>      <td>0.174983</td>      <td>-0.022692</td>      <td>0.108434</td>      <td>-0.045435</td>      <td>0.027162</td>      <td>0.104104</td>      <td>-0.084211</td>      <td>-0.004777</td>      <td>0.062626</td>      <td>0.087100</td>      <td>0.255805</td>      <td>0.069316</td>      <td>-0.109844</td>      <td>-0.037226</td>      <td>-0.150164</td>      <td>0.098879</td>      <td>-0.104635</td>      <td>0.072455</td>      <td>-0.061474</td>      <td>0.169599</td>      <td>-0.016384</td>      <td>0.084960</td>      <td>-0.116534</td>      <td>0.067493</td>      <td>0.043520</td>      <td>0.114401</td>      <td>-0.176764</td>      <td>0.048813</td>      <td>0.041205</td>      <td>-0.081188</td>      <td>-0.042701</td>      <td>-0.205614</td>      <td>-0.236514</td>      <td>-0.043549</td>      <td>0.001771</td>      <td>0.065131</td>      <td>-0.041330</td>      <td>-0.099610</td>      <td>0.023299</td>      <td>0.128153</td>      <td>-0.010431</td>      <td>0.011199</td>      <td>0.156916</td>      <td>0.048151</td>      <td>-0.135613</td>      <td>-0.016057</td>      <td>0.101226</td>      <td>0.102928</td>      <td>-0.002512</td>      <td>-0.002760</td>      <td>0.020232</td>      <td>0.029930</td>      <td>-0.062793</td>      <td>0.045206</td>      <td>-0.223776</td>      <td>0.048469</td>      <td>0.121942</td>      <td>0.100234</td>      <td>-0.103865</td>      <td>0.294303</td>      <td>0.048940</td>      <td>-0.216758</td>      <td>-0.026033</td>      <td>-0.063984</td>      <td>0.148689</td>      <td>0.185192</td>      <td>0.121108</td>      <td>0.023597</td>      <td>-0.029472</td>      <td>0.056568</td>      <td>-0.042976</td>      <td>0.062583</td>      <td>0.148316</td>      <td>0.110256</td>      <td>0.087302</td>      <td>-0.009516</td>      <td>-0.040689</td>      <td>-0.011972</td>      <td>-0.057546</td>      <td>0.168177</td>      <td>-0.091287</td>      <td>-0.023582</td>      <td>0.159975</td>      <td>-0.036707</td>      <td>0.006189</td>      <td>0.070295</td>      <td>-0.129578</td>      <td>0.122715</td>      <td>0.050098</td>      <td>-0.094015</td>      <td>0.063526</td>      <td>0.099861</td>      <td>0.052234</td>      <td>0.016041</td>      <td>-0.042577</td>      <td>-0.043660</td>    </tr>  </tbody></table>

Since multiple news descriptions may be related to the same event, let's perform clustering so that we can group theses pieces together. Multiple clustering algorithm can be applied here, I chose to use DBScan since it does not require us to identify how many cluster there is. Instead, we have to experiment with and finetune the epsilon value to get the most number of clusters and to reduce noise. After finding the most optimal value for epsilon, the algorithm identifies **7 clusters** and **12 unclustered** descriptions, shown as below, where the same colored circles represent the same event and the black circles represent the unclustered articles:<br/>
![event_cluster](https://user-images.githubusercontent.com/30851539/62403414-eed2fe80-b55a-11e9-8c22-3b50a218ebb6.png)<br/>
Let's now explore what articles each of the numbers correspond to:
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Event</th>      <th>Description</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>-1</td>      <td>law protects consumers data written age hacking</td>    </tr>    <tr>      <th>2</th>      <td>-1</td>      <td>antitrust regulators raided offices automaker bmw munich company said friday fresh blow beleaguered german car industry</td>    </tr>    <tr>      <th>18</th>      <td>-1</td>      <td>reason dieselgate class action australia says day looms volkswagen australia several court actions ramp</td>    </tr>    <tr>      <th>17</th>      <td>-1</td>      <td>mysterious green line appeared side iphone screen alone</td>    </tr>    <tr>      <th>7</th>      <td>-1</td>      <td>small number users say annoying line popped screen people claiming glitch permanent</td>    </tr>    <tr>      <th>29</th>      <td>-1</td>      <td>missile defense agency finished placing interceptors alaskan military base</td>    </tr>    <tr>      <th>9</th>      <td>-1</td>      <td>dragonball fighterz roster grows new characters massive new expansion revealed path exile</td>    </tr>    <tr>      <th>11</th>      <td>-1</td>      <td>reason believe north stopped developing war weapons according shinzo abe</td>    </tr>    <tr>      <th>27</th>      <td>-1</td>      <td>car sales returned growth europe last month latest industry data showed tod</td>    </tr>    <tr>      <th>16</th>      <td>-1</td>      <td>regime remained quiet even trump travels asia</td>    </tr>    <tr>      <th>14</th>      <td>-1</td>      <td>food</td>    </tr>    <tr>      <th>30</th>      <td>-1</td>      <td>gunman went rampage northern california deranged paranoid killer bail assaulting two neighbors year authorities say</td>    </tr>    <tr>      <th>0</th>      <td>0</td>      <td>president donald trump said monday japan would shoot north korean missiles sky bought weaponry needed suggesting tokyo take stance avoided</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>zimbabwe president robert mugabe meeting south african delegation state house today negotiations pushed resolution political turmoil likely end decades long rule</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>nothing going change zimbabwe president robert mugabe military leaders seized power made decisions together last years</td>    </tr>    <tr>      <th>20</th>      <td>0</td>      <td>south korea united states agreed friday keep working peaceful end north korean nuclear crisis envoy said difficult gauge reclusive north intentions signal</td>    </tr>    <tr>      <th>28</th>      <td>1</td>      <td>even fulfills promises game may still far gone dark side</td>    </tr>    <tr>      <th>5</th>      <td>1</td>      <td>star wars battlefront releases tomorrow game even made official debut raising kinds havoc players complaining time money would take unlock everything via microtransactions tried revamp system lower hero</td>    </tr>    <tr>      <th>21</th>      <td>1</td>      <td>response gamers frustrations slashes amount game credits needed top characters making darth vader lot affordable</td>    </tr>    <tr>      <th>6</th>      <td>2</td>      <td>volkswagen vowg_p yet fix around million cars affected diesel emissions scandal britain two years since revelations first came light according parliamentary committee</td>    </tr>    <tr>      <th>24</th>      <td>2</td>      <td>diesel emissions scandal already cost volkswagen billion euros end sight start corrugated iron shack forests west virginia discovered trio university students</td>    </tr>    <tr>      <th>13</th>      <td>2</td>      <td>dieselgate emissions scandal scandal continued hold back volkswagen group hitting profits despite carmaker posting strong sales third quarter year</td>    </tr>    <tr>      <th>15</th>      <td>3</td>      <td>north korea gone two months without test firing missile longest dry spell year reason hope</td>    </tr>    <tr>      <th>8</th>      <td>3</td>      <td>north korea fired missile days may winter training cycle pyongyang easing provocati</td>    </tr>    <tr>      <th>26</th>      <td>4</td>      <td>ohio man threatened kill wife carry mass shootings las vegas casino church would history fbi said thursday uncovering series text messages</td>    </tr>    <tr>      <th>10</th>      <td>4</td>      <td>las vegas police identified man officer shot killed south valley saturday evening</td>    </tr>    <tr>      <th>19</th>      <td>5</td>      <td>volkswagen may subject new allegation insider information disclosure dieselgate emissions scam remains headlines two years following disclosure</td>    </tr>    <tr>      <th>12</th>      <td>5</td>      <td>late last month equifax secured control domains mimicking website company launched september wake massive data breach</td>    </tr>    <tr>      <th>31</th>      <td>5</td>      <td>executives equifax first earnings call since disclosed massive breach september detailed challenging environment company works overhaul security measures</td>    </tr>    <tr>      <th>23</th>      <td>6</td>      <td>reports emerging abnormal green lines appearing edges latest iphone first apple phone use organic light emitting displays industry watchers questioning whether problem due quality active matrix organic light emitting diode screens exclusively supplied samsung display apples incapability adopt manufacture advanced screen phones</td>    </tr>    <tr>      <th>25</th>      <td>6</td>      <td>days launch iphone problems starting emerge apple latest handset users reporting various issues screen including fact becomes unresponsive cold conditions</td>    </tr>    <tr>      <th>22</th>      <td>6</td>      <td>apple reportedly replacing affected iphone units new one free cost</td>    </tr>  </tbody></table>

Now to extract the event, we can pick out the "hero" story which is the article that is the closest to the cluster center. Let's pick event 0 as an example.
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>date_publish</th>      <th>description</th>      <th>event_label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>2017-11-06 22:18:21</td>      <td>U.S. President Donald Trump said on Monday that Japan would shoot North Korean missiles "out of the sky" if it bought the U.S. weaponry needed for doing so, suggesting Tokyo take a stance it has avoided until now.</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>2017-11-16 00:00:00</td>      <td>Zimbabwe\'s President Robert Mugabe was meeting with a South African delegation at the state house today as negotiations pushed for a resolution to the political turmoil and the likely end to his decades-long rule.</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>2017-11-17 00:00:00</td>      <td>Nothing is going to change in Zimbabwe‚ because President Robert Mugabe and the military leaders who seized power made decisions together for the last 37 years.</td>      <td>0</td>    </tr>    <tr>      <th>20</th>      <td>2017-11-17 12:49:05</td>      <td>South Korea and the United States agreed on Friday to keep working for a peaceful end to the North Korean nuclear crisis, but a U.S. envoy said it was difficult to gauge the reclusive North\'s intentions as there has been "no signal".</td>      <td>0</td>    </tr>  </tbody></table>

As we can see, this technique was able to pick up articles talking about the same event. However, more fine-tuning is definitely needed since it classifies the presidential news in Zimbabwe and news related to President Donald Trump as the same event. And the "hero story" in this case is 
<pre><code> U.S. President Donald Trump said on Monday that Japan would shoot North Korean missiles "out of the sky" if it bought the U.S. weaponry needed for doing so, suggesting Tokyo take a stance it has avoided until now.</code></pre>

## Text Summarization (single document vs. multidocument) ```https://arxiv.org/pdf/1903.10318.pdf```
Summarization is the task of producing a shorter version of the original text while preserving its context and meaning. Since text summarization is a rather subjective task, using the automatic metrics such as ROUGE and METEOR have limitations to evaluate the fluency, grammaticality and coherence of the summarized text. Therefore, we cannot simply claim a state-of-art model based sole on the evaluation scores.
### 1. Extractive Summarization (Allahyari et al., 2017)
Extractive summarization approach selects a subset of important sentences from the original text. All summarizers perform the following 3 independent tasks:
``` 
1. Construct an intermediate representatoin of of the input text
2. Score the sentences based on the representation
3. Select a summary comprising of the key sentences based on highest scoring.
```
Though many neural models have been proposed for extractive summarization, one of the very recent paper that was published on March 25th, 2019 claims that a fine-tuned BERT(Bidirectional Encoder Representations from Transformers) model achieved state-of-art performance. For more information on the original BERT, see this Github repository: https://github.com/google-research/bert 
For more information on the fine-tuned BERT model, please refer here: https://github.com/nlpyang/BertSum
To illustrate how text summarization works, we will use the same news dataset as before.
#### Feature based approach
#### Graph based approach
The **Gensim** library uses the graph based approach to do text summarization. Here is text that we want to summarize:
```TOKYO (Reuters) - U.S. President Donald Trump said on Monday that Japan would shoot North Korean missiles “out of the sky” if it bought the U.S. weaponry needed for doing so, suggesting Tokyo take a stance it has avoided until now.North Korea is pursuing nuclear weapons and missile programs in defiance of U.N. Security Council sanctions and has made no secret of its plans to develop a missile capable of hitting the U.S. mainland. It has fired two missiles over Japan.Trump, speaking after a summit with Japanese Prime Minister Shinzo Abe, repeated his mantra the “era of strategic patience” with North Korea was over, and said the two countries were working to counter the “dangerous aggressions”.Trump also pressed Japan to lower its trade deficit with the United States and buy more U.S. military hardware.“He (Abe) will shoot them out of the sky when he completes the purchase of lots of additional military equipment from the United States,“ Trump said, referring to the North Korean missiles. ”The prime minister is going to be purchasing massive amounts of military equipment, as he should. And we make the best military equipment by far.”Abe, for his part, said Tokyo would shoot down missiles “if necessary”.Trump was replying to a question that was posed to Abe - namely how he would respond to a quote from Trump from a recent interview in which he said Japan was a “samurai” nation and should have shot down the North Korean missiles.Japan’s policy is that it would only shoot down a missile if it were falling on Japanese territory or if it were judged to pose an “existential threat” to Japan because it was aimed at a U.S. target.The U.S. president is on the second day of a 12-day Asian trip that is focusing on North Korea’s nuclear missile programs and trade.“Most importantly, we’re working to counter the dangerous aggressions of the regime in North Korea,” Trump said, calling Pyongyang’s nuclear tests and recent launches of ballistic missiles over Japan “a threat to the civilized world and to international peace and stability”.“Some people said that my rhetoric is very strong. But look what’s happened with very weak rhetoric over the last 25 years. Look where we are right now,” he added.North Korea’s recent actions have raised the stakes in the most critical international challenge of Trump’s presidency.The U.S. leader, who will visit South Korea on the trip, has rattled some allies with his vow to “totally destroy” North Korea if it threatens the United States and with his dismissal of North Korean leader Kim Jong Un as a “rocket man” on a suicide mission.Abe, with whom Trump has bonded through multiple summits and phone calls, repeated at the same news conference that Japan backed Trump’s stance that “all options” are on the table, saying it was time to exert maximum pressure on North Korea and the two countries were “100 percent” together on the issue.Chinese Foreign Ministry spokesman Hua Chunying, in response to Abe’s comments, said that the North Korean “situation” was “already extremely complex, sensitive and weak”.“We hope that under the present circumstances, all sides’ words and actions can help reduce tensions and reestablish mutual trust and getting the North Korean nuclear issue back on the correct track of dialogue and negotiations,” she said.TRADE DEFICITSWaiters hold U.S. President Donald Trump's Diet Coke as he delivers remarks before his toast to Japan's Prime Minister Shinzo Abe during an official dinner in Trump's honor at Akasaka Palace in Tokyo, Japan November 6, 2017. REUTERS/Jonathan ErnstTrump said he was committed to achieving “free, fair, and reciprocal” trade and wants to work with Japan on this issue.“America is also committed to improving our economic relationship with Japan,” Trump said. “As president, I‘m committed to achieving fair, free, and reciprocal trading relationship. We seek equal and reliable access for American exports to Japan’s markets in order to eliminate our chronic trade imbalances and deficits with Japan.”Earlier, speaking to Japanese and U.S. business executives, Trump praised Japan for buying U.S. military hardware.But he added that “many millions of cars are sold by Japan into the United States, whereas virtually no cars go from the United States into Japan”.Japan had a $69 billion trade surplus with the United States last year, according to the U.S. Treasury Department. The United States was Japan’s second biggest trade partner after China, while Japan was the United States’ fourth largest goods export market in 2016.EMPEROR, ABDUCTEESSlideshow (25 Images)Japanese officials have countered U.S. trade complaints by noting Tokyo accounts for a much smaller slice of the U.S. deficit than in the past, while China’s imbalance is bigger.In a second round of economic talks in Washington last month, U.S. Vice President Mike Pence and Japanese Finance Minister Taro Aso, who doubles as deputy premier, failed to bridge differences on trade issues.The two sides are at odds over how to frame future trade talks, with Tokyo pushing back against U.S. calls to discuss a bilateral free trade agreement.Trump also said earlier that an Indo-Pacific trade framework would produce more in trade that the Trans-Pacific Partnership pact pushed by his predecessor but which he announced Washington would abandon soon after he took office.The 11 remaining nations in the TPP, to which Japan’s Abe is firmly committed, are edging closer to sealing a comprehensive free trade pact without the United States.Trump met Emperor Akihito, exchanging a handshake and nodding, before his lunch and talks with Abe.He also met relatives of Japanese citizens abducted by North Korean agents decades ago to help train spies, calling the kidnappings a “tremendous disgrace” and pledging to work with Abe to bring the victims “back to Japan where they want to be”.“I think it would be a tremendous signal if Kim Jong Un would send them back,” Trump said. “If he would send them back, that would be the start of something, something very special.”Abe has made resolving the emotive abductions issue a keystone of his career. The families hope their talks with Trump - the third U.S. president they have met - will somehow contribute to a breakthrough, although experts say progress is unlikely.Abe also expressed his condolences for the victims of a gunman who massacred at least 26 worshippers at a church in Texas.White House spokeswoman Sarah Sanders said Trump had no plans to change the schedule for his 12-day Asian trip, which will also take him to Seoul, Beijing and Danang, Vietnam.```
</br>
Here is the summary:
```It has fired two missiles over Japan.Trump, speaking after a summit with Japanese Prime Minister Shinzo Abe, repeated his mantra the “era of strategic patience” with North Korea was over, and said the two countries were working to counter the “dangerous aggressions”.Trump also pressed Japan to lower its trade deficit with the United States and buy more U.S. military hardware.“He (Abe) will shoot them out of the sky when he completes the purchase of lots of additional military equipment from the United States,“ Trump said, referring to the North Korean missiles.\nAnd we make the best military equipment by far.”Abe, for his part, said Tokyo would shoot down missiles “if necessary”.Trump was replying to a question that was posed to Abe - namely how he would respond to a quote from Trump from a recent interview in which he said Japan was a “samurai” nation and should have shot down the North Korean missiles.Japan’s policy is that it would only shoot down a missile if it were falling on Japanese territory or if it were judged to pose an “existential threat” to Japan because it was aimed at a U.S. target.The U.S. president is on the second day of a 12-day Asian trip that is focusing on North Korea’s nuclear missile programs and trade.“Most importantly, we’re working to counter the dangerous aggressions of the regime in North Korea,” Trump said, calling Pyongyang’s nuclear tests and recent launches of ballistic missiles over Japan “a threat to the civilized world and to international peace and stability”.“Some people said that my rhetoric is very strong.```

#### Topic representation approaches
#### Knowledge bases and automatic summarization
### 2. Abstractive Summarization
Contrary to the extractive approach, abstractive summarization is closer to how human would perform the task of summarization. In this paradigm, the summarized text contains words and phrases that were not necessarily in the original text, it is able to synthesize and generate a summary based on the original text. 
Neural sequence-to-sequence models are the most common approach for abstractive text summarization.

## Machine Translation (MT) (https://nlp.stanford.edu/projects/mt.shtml) (https://arxiv.org/pdf/1901.01122.pdf)
Machine translation is one of the oldest AI subfields. It refers to the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. 
### 1. Rule based MT
This is the primary research focus in 1970s. This approach consists of the following steps:
```
1. Converting the source text to a language-free conceptual representation.
2. Augmenting this representation with information that was implicit in the source text.
3. Converting previously augmented representation to the target language
```

### 2. Statistical based MT
In most statistical approches, the most crucial component of the system is the phrase transition model. 

### 3. Neural based MT

## Opinion Mining
Sentiment analysis is one of the most known subfields of opinion mining. It is widely applied to reviews, survey responses in domains ranging from marketing to clinical medicine. Different classifiers can be employed to perform sentiment analysis. More traditional methods include Naive Bayes, Support Vector Machine, etc. Currently, deep learning techniques perform the best on sentiment analysis.
